{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Art with more than one style image\n",
    "\n",
    "In diesem Projekt wollen wir uns mit dem Erstellen von Bildern aus Content- und Style- Bildern mithilfe von maschinellem Lernen beschäftigen. \n",
    "\n",
    "Als Grundlage dieser Arbeit dient [dieses Notebook von Harish Narayanan](https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/6_Artistic_style_transfer_with_a_repurposed_VGG_Net_16.ipynb).\n",
    "Wir erweitern dieses um unsere eigenen Gedanken, und führen die Option ein, mehrere Style- Bilder zu verwenden. Am Ende zeigen wir unsere eigenen Bilder und welchen Einfluss die verschiedenen Parameter haben.\n",
    "\n",
    "Als Frameworks nutzen wir `keras`, welches Tensorflow (1.0) als Backend benutzt, sowie `scypy` für einen Optimierungsalgorithmus sowie `PIL`, um Bilder laden und bearbeiten zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst geben wir die Größe der Bilder an. Egal, welche Größe die Ausgangsbilder haben, sie werden alle auf diese Größe skaliert.\n",
    "\n",
    "Um maschinelles Lernen betreiben zu können, werden die Bilder in Numpy-Arrays konvertiert. Es wird eine Dimension hinzugefügt, damit am Ende alle Bilder in einem Datenobjekt zusammengefasst werden können.\n",
    "\n",
    "Wir werden das viel verwendete VGG16-Modell benutzen. Da dieses Modell ein spezielles Datenformat verlangt, müssen die Daten noch folgendermaßen angepasst werden:\n",
    "* Von allen Pixeln werden die mittleren RGB-Werte in den entsprechenden Farbchannels abgezogen.\n",
    "* Die Farbchannels werden in ihrer Reihenfolge umgekehrt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 512\n",
    "width = 512\n",
    "\n",
    "content_image_paths = [ \"images/bild_content_quadratisch.jpg\" ]\n",
    "content_imgs = []\n",
    "for content_img_path in content_image_paths:\n",
    "    content_img = Image.open(content_img_path)\n",
    "    content_img = content_img.resize((height, width))\n",
    "    content_arr = np.asarray(content_img, dtype='float32')\n",
    "    content_arr = np.expand_dims(content_arr, axis=0)\n",
    "    content_arr[:, :, :, 0] -= 103.939\n",
    "    content_arr[:, :, :, 1] -= 116.779\n",
    "    content_arr[:, :, :, 2] -= 123.68\n",
    "    # Convert from RGB to BGR\n",
    "    content_arr = content_arr[:, :, :, ::-1]\n",
    "    content_img = backend.variable(content_arr)\n",
    "    content_imgs.append(content_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image_paths = ['images/styles/wave.jpg']\n",
    "style_imgs = []\n",
    "for style_img_path in style_image_paths:\n",
    "    style_img = Image.open(style_img_path)\n",
    "    style_img = style_img.resize((height, width))\n",
    "    style_arr = np.asarray(style_img, dtype='float32')\n",
    "    style_arr = np.expand_dims(style_arr, axis=0)\n",
    "    style_arr[:, :, :, 0] -= 103.939\n",
    "    style_arr[:, :, :, 1] -= 116.779\n",
    "    style_arr[:, :, :, 2] -= 123.68\n",
    "    # Convert from RGB to BGR\n",
    "    style_arr = style_arr[:, :, :, ::-1]\n",
    "    style_img = backend.variable(style_arr)\n",
    "    style_imgs.append(style_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem Falle sind wir nicht daran interessiert, das Klassifikationsnetzwerk zur tatsächlichen Klassifikation zu benutzen, sondern wollen statt Weights und Biases anzupassen unsere Inputs anpassen, um die Kostenfunktion zu minimieren (diese wird später genauer erläutert). Dafür brauchen wir in den Inputs neben den Ausgangsbildern noch Platz für das Ergebnisbild. Dafür erstellen wir einen Placeholder, der dieselben Dimensionen hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels as the last dimension, using backend Tensorflow\n",
    "combination_img = backend.placeholder((1, height, width, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir konkatenieren alle Ausgangsbilder und den Platzhalter für das Ergebnisbild in einen Tensor, der dem Modell als Input dient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = []\n",
    "for content_img in content_imgs:\n",
    "    all_imgs.append(content_img)\n",
    "for style_img in style_imgs:\n",
    "    all_imgs.append(style_img)\n",
    "all_imgs.append(combination_img)\n",
    "input_tensor = backend.concatenate(all_imgs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird das Netzwerk initialisiert. Genaueres zum Aufbau in [diesem Notebook](VGG16_mnist.ipynb). Wir nutzen das bereits mit Daten von Imagenet trainierte Netzwerk und lassen die Fully-Connected-Layers weg, da diese nur für die Klassifikation benötigt werden und keine für uns relevanten Informationen enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'block1_conv1': <tf.Tensor 'block1_conv1/Relu:0' shape=(3, 512, 512, 64) dtype=float32>,\n",
       " 'block1_conv2': <tf.Tensor 'block1_conv2/Relu:0' shape=(3, 512, 512, 64) dtype=float32>,\n",
       " 'block1_pool': <tf.Tensor 'block1_pool/MaxPool:0' shape=(3, 256, 256, 64) dtype=float32>,\n",
       " 'block2_conv1': <tf.Tensor 'block2_conv1/Relu:0' shape=(3, 256, 256, 128) dtype=float32>,\n",
       " 'block2_conv2': <tf.Tensor 'block2_conv2/Relu:0' shape=(3, 256, 256, 128) dtype=float32>,\n",
       " 'block2_pool': <tf.Tensor 'block2_pool/MaxPool:0' shape=(3, 128, 128, 128) dtype=float32>,\n",
       " 'block3_conv1': <tf.Tensor 'block3_conv1/Relu:0' shape=(3, 128, 128, 256) dtype=float32>,\n",
       " 'block3_conv2': <tf.Tensor 'block3_conv2/Relu:0' shape=(3, 128, 128, 256) dtype=float32>,\n",
       " 'block3_conv3': <tf.Tensor 'block3_conv3/Relu:0' shape=(3, 128, 128, 256) dtype=float32>,\n",
       " 'block3_pool': <tf.Tensor 'block3_pool/MaxPool:0' shape=(3, 64, 64, 256) dtype=float32>,\n",
       " 'block4_conv1': <tf.Tensor 'block4_conv1/Relu:0' shape=(3, 64, 64, 512) dtype=float32>,\n",
       " 'block4_conv2': <tf.Tensor 'block4_conv2/Relu:0' shape=(3, 64, 64, 512) dtype=float32>,\n",
       " 'block4_conv3': <tf.Tensor 'block4_conv3/Relu:0' shape=(3, 64, 64, 512) dtype=float32>,\n",
       " 'block4_pool': <tf.Tensor 'block4_pool/MaxPool:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv1': <tf.Tensor 'block5_conv1/Relu:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv2': <tf.Tensor 'block5_conv2/Relu:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv3': <tf.Tensor 'block5_conv3/Relu:0' shape=(3, 32, 32, 512) dtype=float32>,\n",
       " 'block5_pool': <tf.Tensor 'block5_pool/MaxPool:0' shape=(3, 16, 16, 512) dtype=float32>,\n",
       " 'input_1': <tf.Tensor 'concat:0' shape=(3, 512, 512, 3) dtype=float32>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man sehen kann, schlägt sich die Bildgröße in allen Schichten in der Parameteranzahl wider. Dies bedeutet, dass eine Erhöhung der Auflösung zu einer deutlich überproportional längeren Berechnungsdauer führt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein möglichst gutes Endergebnis zu erhalten, muss dafür gesorgt werden, dass sowohl der Inhalt der Content-Bilder erhalten bleibt, als auch der Style der Style-Bilder angemessen im Ergebnisbild zu erkennen ist. Da diese mit einander in Konflikt stehen, nutzen wir als Loss eine gewichtete Summe. \n",
    "\n",
    "Im Folgenden werden die einzelnen Bestandteile der Kostenfunktion genauer erläutert. \n",
    "\n",
    "Die Werte der Gewichte haben wir aus [Narayanans Notebook](https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/6_Artistic_style_transfer_with_a_repurposed_VGG_Net_16.ipynb) übernommen. Durch viel Ausprobieren fand er diese Werte, die zu einem subjektiv als gut empfundenen Ergebnis geführt haben. Es bedeutet nicht, dass der Style 200x wichtiger als der Content ist, da die Ergebnisse der einzelnen Loss-Funktionen nicht auf eine Verteilung normalisiert werden und somit nicht direkt miteinander vergleichbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 0.025\n",
    "style_weight = 5.0\n",
    "tv_weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = backend.variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Content-Loss ist die euklidische Distanz zwischen den Outputs einer ausgewählten Schicht für den Content und unser Ergebnisbild. \n",
    "\n",
    "Die höheren Schichten erkennen eher größere Strukturen und weniger konkrete Details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, combination):\n",
    "    return backend.sum(backend.square(combination - content))\n",
    "\n",
    "# Add content loss to this layer\n",
    "layer_features = layers['block2_conv2']\n",
    "\n",
    "content_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[len(content_imgs) + len(style_imgs), :, :, :]\n",
    "\n",
    "loss += content_weight * content_loss(content_features, combination_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Style-Loss ist der interessanteste Teil dieses Algorithmus. Hier wird eine sogenannte Gram-Matrix zwischengeschaltet, die die Korrelation der Outputs der Neuronen verschiedener Schichten für alle Style-Bilder repräsentiert. Somit werden Informationen über die einzelnen Outputs verloren, aber Informationen über die Zusammenhänge der Neuronen untereinander gewonnen. Dies ist ein erstaunlich guter Indikator für Stil-Ähnlichkeit.\n",
    "\n",
    "In der Gram Matrix werden alle Outputs der betroffenen Neuronen paarweise miteinander multipliziert. Dafür wird zunächst ein Spaltenvektor aus allen n Outputs generiert und dieser mit seiner Transposition multipliziert, sodass eine n x n - Matrix entsteht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = backend.dot(features, backend.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = height * width\n",
    "    return backend.sum(backend.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "feature_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = layers[layer_name]\n",
    "    for style_img_idx in range(len(style_imgs)):\n",
    "        style_features = layer_features[1 + style_img_idx, :, :, :]\n",
    "        combination_features = layer_features[len(content_imgs) + len(style_imgs), :, :, :]\n",
    "        style_l = style_loss(style_features, combination_features)\n",
    "        loss += (style_weight / (len(feature_layers)*len(style_imgs))) * style_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit das Ergebnisbild nicht zu verrauscht ist, wird noch ein dritter Term in die Loss-Funktion aufgenommen: der Total Variation Loss. Hierbei wird einmal das Bild ohne letzte Zeile über das Bild ohne erste Zeile gelegt und die euklidische Distanz berechnet. Für die Spalten wird analog vorgegangen und das Ergebnis aufaddiert. So wird insgesamt jeder Pixel einmal mit jedem direkten Nachbarn (oben, unten, rechts, links) verglichen und zu große Unterschiede werden bestraft. Somit wird einem zu großen Rauschen entgegengewirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total variation loss to ensure the image is smooth and continuous throughout\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n",
    "    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n",
    "    return backend.sum(backend.pow(a + b, 1.25))\n",
    "\n",
    "loss += tv_weight * total_variation_loss(combination_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden findet der Lernprozess statt. Wichtig ist, dass die Variablen `loss` (von oben: `backend.variable`), `grads` und `f_outputs` die jeweiligen `backend`-Funktionen nutzen, damit sie in jeder Iteration neu evaluiert werden können. Der Evaluator wurde als Klasse implementiert, damit die Funktion `fmin_l_bfgs_b` in der for-Schleife die Werte von `loss` und `grads` an zwei Stellen übergeben werden können, ohne zweimal berechnet werden zu müssen (sie werden nur bei `Evaluator.loss()` berechnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = backend.gradients(loss, combination_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [loss]\n",
    "outputs += grads\n",
    "# Create the function from input combination_img to the loss and gradients\n",
    "f_outputs = backend.function([combination_img], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We finally have the gradients and losses at the combination_img computed as variables\n",
    "# and we can use any standard optimization function to optimize combination_img\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, height, width, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1].flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start of iteration', 0)\n",
      "('Current loss value:', 3.4901606e+10)\n",
      "Iteration 0 completed in 1065s\n",
      "('Start of iteration', 1)\n",
      "('Current loss value:', 1.8784664e+10)\n",
      "Iteration 1 completed in 879s\n",
      "('Start of iteration', 2)\n",
      "('Current loss value:', 1.2682997e+10)\n",
      "Iteration 2 completed in 905s\n",
      "('Start of iteration', 3)\n",
      "('Current loss value:', 1.0791149e+10)\n",
      "Iteration 3 completed in 902s\n",
      "('Start of iteration', 4)\n",
      "('Current loss value:', 9.7563412e+09)\n",
      "Iteration 4 completed in 872s\n",
      "('Start of iteration', 5)\n",
      "('Current loss value:', 9.148119e+09)\n",
      "Iteration 5 completed in 918s\n",
      "('Start of iteration', 6)\n",
      "('Current loss value:', 8.7887421e+09)\n",
      "Iteration 6 completed in 909s\n",
      "('Start of iteration', 7)\n",
      "('Current loss value:', 8.5374116e+09)\n",
      "Iteration 7 completed in 942s\n",
      "('Start of iteration', 8)\n",
      "('Current loss value:', 8.3602642e+09)\n",
      "Iteration 8 completed in 907s\n",
      "('Start of iteration', 9)\n",
      "('Current loss value:', 8.2301092e+09)\n",
      "Iteration 9 completed in 898s\n"
     ]
    }
   ],
   "source": [
    "# random first image\n",
    "x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.0\n",
    "\n",
    "# in order for the output to be readable for humans, \n",
    "# the transformations done in the beginning have to be undone before saving.\n",
    "def transform_back(x):\n",
    "    x1 = copy.deepcopy(x)\n",
    "    x1 = x1.reshape((height, width, 3))\n",
    "    # Convert back from BGR to RGB to display the image\n",
    "    x1 = x1[:, :, ::-1]\n",
    "    x1[:, :, 0] += 103.939\n",
    "    x1[:, :, 1] += 116.779\n",
    "    x1[:, :, 2] += 123.68\n",
    "    x1 = np.clip(x1, 0, 255).astype('uint8')\n",
    "    return Image.fromarray(x1)\n",
    "\n",
    "transform_back(x).save('Galerie/before.bmp')\n",
    "\n",
    "iters = 10\n",
    "\n",
    "for i in range(iters):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
    "\n",
    "    transform_back(x).save('Galerie/result' + str(i) + '.bmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im Folgenden wollen wir auf die Ergebnisse für verschiedene Parameterwerte eingehen. Wir haben jeweils alle Werte so gelassen wie oben zu sehen, und einen Wert verändert, um deren Einfluss zu studieren. Links ist immer ein gif zu sehen, welches die Entwicklung zeigt, und rechts das Ergebnisbild der letzten Iteration. Für einen besseren Gesamtüberblick sind die Bilder hier verkleinert dargestellt. Im Galerie-Ordner kann man sie sich in 512x512 anschauen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folgende Bilder haben wir als Inputs genommen:\n",
    "    \n",
    "Content: <img src=\"images/bild_content_quadratisch.jpg\" alt=\"content\" width=\"256\"/> Style: <img src=\"images/styles/wave.jpg\" alt=\"content\" width=\"256\"/>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst das Bild mit den Standard-Parametern:\n",
    "\n",
    "<img src=\"Galerie/standard/result.gif\" width=\"256\"/>    <img src=\"Galerie/standard/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/standard/result.gif) ![standard](Galerie/standard/result9.bmp)\n",
    "-->\n",
    "Wie man sieht, ist das Ergebnis ein guter Kompromiss zwischen allen Parametern.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwei Styles:\n",
    "\n",
    "<img src=\"Galerie/1_content_2_styles/result.gif\" width=\"256\"/>    <img src=\"Galerie/1_content_2_styles/result7.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/1_content_2_styles/result.gif) ![standard](Galerie/1_content_2_styles/result7.bmp)\n",
    "-->\n",
    "___\n",
    "\n",
    "Hier wurden zwei Style-Bilder übergeben. Zum einen, dass im Standard definierte und als zweites dieses hier:\n",
    "\n",
    "<img src=\"images/styles/marilyn.jpg\"  width=\"256\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`content_weight = 0.5 # Standard: 0.025`\n",
    "\n",
    "<img src=\"Galerie/content_weight_0-5/result.gif\" width=\"256\"/>    <img src=\"Galerie/content_weight_0-5/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/content_weight_0-5/result.gif) ![standard](Galerie/content_weight_0-5/result9.bmp)\n",
    "-->\n",
    "Hier kommt der Style kaum zur Geltung. Er schlägt sich fast nur in den Farbtönen nieder.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`style_weight = 15 # Standard: 5`\n",
    "\n",
    "<img src=\"Galerie/style_weight_15/result.gif\" width=\"256\"/>    <img src=\"Galerie/style_weight_15/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/style_weight_15/result.gif) ![standard](Galerie/style_weight_15/result9.bmp)\n",
    "-->\n",
    "Hier kommt auch ein sehr ansprechendes, noch verzierteres Bild heraus.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tv_weight = 0 # Standard: 1`\n",
    "\n",
    "<img src=\"Galerie/tv_weight_0/result.gif\" width=\"256\"/>    <img src=\"Galerie/tv_weight_0/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/tv_weight_0/result.gif) ![standard](Galerie/tv_weight_0/result9.bmp)\n",
    "-->\n",
    "Das Rauschen aus dem Anfangs-Zufallsbild wird ohne total-variation-loss wenig abgemildert, sodass das Ergebnis sehr verrauscht ist.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tv_weight = 10 # Standard: 1`\n",
    "\n",
    "<img src=\"Galerie/tv_weight_10/result.gif\" width=\"256\"/>    <img src=\"Galerie/tv_weight_10/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/tv_weight_10/result.gif) ![standard](Galerie/tv_weight_10/result9.bmp)\n",
    "-->\n",
    "Hier entstehen viele klar abgegrenzte Flächen gleicher Farbe und weniger Farbübergänge. Der Himmel zeigt eigenartigerweise mehr Struktur als im Standard-Bild.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tv_weight = 100 # Standard: 1`\n",
    "\n",
    "<img src=\"Galerie/tv_weight_100/result.gif\" width=\"256\"/>    <img src=\"Galerie/tv_weight_100/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/tv_weight_100/result.gif) ![standard](Galerie/tv_weight_100/result9.bmp)\n",
    "-->\n",
    "Hier wird das Rauschen so stark verhindert, dass das im Ergebnis kaum Feinheiten zu erkennen sind, sondern nur noch größere Farbpunkte.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer_features = layers['block1_conv1'] # Standard: layers['block2_conv2']`\n",
    "\n",
    "<img src=\"Galerie/layer_features_block1_conv1/result.gif\" width=\"256\"/>    <img src=\"Galerie/layer_features_block1_conv1/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/layer_features_block1_conv1/result.gif) ![standard](Galerie/layer_features_block1_conv1/result9.bmp)\n",
    "-->\n",
    "Im ersten convolutional layer scheinen die vom Netzwerk erkannten Content-Strukturen noch so detailliert zu sein, dass hier für uns Menschen nichts zu erkennen ist.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer_features = layers['block3_conv1'] # Standard: layers['block2_conv2']`\n",
    "\n",
    "<img src=\"Galerie/layer_features_block3_conv1/result.gif\" width=\"256\"/>    <img src=\"Galerie/layer_features_block3_conv1/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/layer_features_block3_conv1/result.gif) ![standard](Galerie/layer_features_block3_conv1/result9.bmp)\n",
    "-->\n",
    "Dieses Ergebnis ist dem Standard sehr ähnlich. Es ist ein ähnlicher Effekt, wie den style_weight zu erhöhen. Dies lässt darauf schließen, dass hier der das Netzwerk schon etwas abstraktere Konzepte verwendet, die für uns Menschen den Content etwas schlechter erkennen lassen. Die folgenden Bilder bestätigen dies.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer_features = layers['block4_conv1']:`\n",
    "\n",
    "<img src=\"Galerie/layer_features_block4_conv1/result.gif\" width=\"256\"/>    <img src=\"Galerie/layer_features_block4_conv1/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/layer_features_block4_conv1/result.gif) ![standard](Galerie/layer_features_block4_conv1/result9.bmp)\n",
    "-->\n",
    "Hier ist der Content nur noch schemenhaft zu erkennen.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer_features = layers['block5_conv3']:`\n",
    "\n",
    "<img src=\"Galerie/layer_features_block5_conv3/result.gif\" width=\"256\"/>    <img src=\"Galerie/layer_features_block5_conv3/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/layer_features_block5_conv3/result.gif) ![standard](Galerie/layer_features_block5_conv3/result9.bmp)\n",
    "-->\n",
    "In der letzten convolutional Layer scheinen die Neuronen so abstrakte Zusammenhänge zu repräsentieren (am Ende muss das Netzwerk Objekte aus allen möglichen Variationen von Winkel, Lichtverhältnissen, teilweise Verdeckung etc. erkennen können), dass bei Verwendung dieser Layer als Content-Indikator für uns Menschen nichts mehr zu erkennen ist. In einem zukünftigen Projekt könnte man untersuchen, ob das Netzwerk bei Verwendung zur Klassifizierung hier dennoch z.B. zwei menschliche Gesichter erkennen würde.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_layers = layers['block1_conv1, block_2_conv1, block3_conv1]:`\n",
    "\n",
    "<img src=\"Galerie/feature_layers/result.gif\" width=\"256\"/>    <img src=\"Galerie/feature_layers/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "<!---\n",
    "![standard](Galerie/feature_layers/result.gif) ![standard](Galerie/feature_layers/result9.bmp)\n",
    "-->\n",
    "\n",
    "Auch bei den style-Layern wollten wir weitere Variationen ausprobieren. Anstatt aus jedem Block den letzten conv-Layer zu verwenden, haben wir hier aus den ersten drei Blöcken den ersten conv-Layer verwendet. Zu erkennen ist, dass die Farben des style-Bildes richtig erkennt werden sowie auch schon einzelne Strukturen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`feature_layers = layers['block4_conv3, 'block5_conv3']`\n",
    "\n",
    "<img src=\"Galerie/feature_layers/block4_conv3,block5_conv3/result.gif\" width=\"256\"/>    <img src=\"Galerie/feature_layers/block4_conv3,block5_conv3/result9.bmp\" width=\"256\"/>\n",
    "\n",
    "Hier werden zu wenige style-Layers verwendet, weswegen im Bild eher wenig des Stylebildes übernommen wird, bis  auf das farbliche im Himmel und im Wasser. Somit lässt sich daraus schließen, dass mindestens 3 Layer verwendet werden sollten und mehr noch einen besseres Resultat in Hinsicht des Styles generiert wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
