{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Art with more then one style image\n",
    "\n",
    "In diesem Projekt wollen wir uns mit dem Erstellen von Bildern aus Content- und Style- Bildern mithilfe von maschinellem Lernen beschäftigen. \n",
    "\n",
    "Als Grundlage dieser Arbeit dient [dieses Notebook von Harish Narayanan](https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/6_Artistic_style_transfer_with_a_repurposed_VGG_Net_16.ipynb).\n",
    "Wir erweitern dieses um unsere eigenen Gedanken, und führen die Option ein, mehrere Content- und Style- Bilder zu verwenden.\n",
    "\n",
    "Als Frameworks nutzen wir `keras`, welches Tensorflow (1.0) als Backend benutzt, sowie `scypy` für einen Optimierungsalgorithmus sowie `PIL`, um Bilder laden und bearbeiten zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst geben wir die Größe der Bilder an. Egal, welche Größe die Ausgangsbilder haben, sie werden alle auf diese Größe skaliert.\n",
    "\n",
    "Um maschinelles Lernen betreiben zu können, werden die Bilder in Numpy-Arrays konvertiert. Es wird eine Dimension hinzugefügt, damit am Ende alle Bilder in einem Datenobjekt zusammengefasst werden können.\n",
    "\n",
    "Wir werden das viel verwendete VGG16-Modell benutzen. Da dieses Modell ein spezielles Datenformat verlangt, müssen die Daten noch folgendermaßen angepasst werden:\n",
    "* Von allen Pixeln werden die mittleren RGB-Werte in den entsprechenden Farbchannels abgezogen.\n",
    "* Die Farbchannels werden in ihrer Reihenfolge umgekehrt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable_12:0' shape=(1, 512, 512, 3) dtype=float32_ref>, <tf.Variable 'Variable_13:0' shape=(1, 512, 512, 3) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "height = 512\n",
    "width = 512\n",
    "\n",
    "content_image_paths = ['images/hulk.jpg', \"images/giger.jpg\"]\n",
    "content_imgs = []\n",
    "for content_img_path in content_image_paths:\n",
    "    content_img = Image.open(content_img_path)\n",
    "    content_img = content_img.resize((height, width))\n",
    "    content_arr = np.asarray(content_img, dtype='float32')\n",
    "    content_arr = np.expand_dims(content_arr, axis=0)\n",
    "    content_arr[:, :, :, 0] -= 103.939\n",
    "    content_arr[:, :, :, 1] -= 116.779\n",
    "    content_arr[:, :, :, 2] -= 123.68\n",
    "    # Convert from RGB to BGR\n",
    "    content_arr = content_arr[:, :, :, ::-1]\n",
    "    content_img = backend.variable(content_arr)\n",
    "    content_imgs.append(content_img)\n",
    "\n",
    "print(content_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image_paths = ['images/style1.jpg', 'images/style2.jpg']\n",
    "style_imgs = []\n",
    "for style_img_path in style_image_paths:\n",
    "    style_img = Image.open(style_img_path)\n",
    "    style_img = style_img.resize((height, width))\n",
    "    style_arr = np.asarray(style_img, dtype='float32')\n",
    "    style_arr = np.expand_dims(style_arr, axis=0)\n",
    "    style_arr[:, :, :, 0] -= 103.939\n",
    "    style_arr[:, :, :, 1] -= 116.779\n",
    "    style_arr[:, :, :, 2] -= 123.68\n",
    "    # Convert from RGB to BGR\n",
    "    style_arr = style_arr[:, :, :, ::-1]\n",
    "    style_img = backend.variable(style_arr)\n",
    "    style_imgs.append(style_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unserem Falle sind wir nicht daran interessiert, das Klassifikationsnetzwerk zur tatsächlichen Klassifikation zu benutzen, sondern wollen statt Weights und Biases anzupassen unsere Inputs anpassen, um die Kostenfunktion zu minimieren. Dafür brauchen wir in den Inputs neben den Ausgangsbildern noch Platz für das Ergebnisbild. Dafür erstellen wir einen Placeholder, der dieselben Dimensionen hat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels as the last dimension, using backend Tensorflow\n",
    "combination_img = backend.placeholder((1, height, width, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir konkatenieren alle Ausgangsbilder und den Platzhalter für das Ergebnisbild in einen Tensor, der dem Modell als Input dient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now finally have the content image variable, style image variables and combination image placeholder\n",
    "# that we will concatenate to build a final input tensor to build our computation graph on top of, essentially\n",
    "# we pass all these inputs to the network as if they are a part of a batch so they are all run parallely and we\n",
    "# use the features generated to modify our combination image based on the losses. TODO: Delete this comment\n",
    "\n",
    "all_imgs = []\n",
    "for content_img in content_imgs:\n",
    "    all_imgs.append(content_img)\n",
    "for style_img in style_imgs:\n",
    "    all_imgs.append(style_img)\n",
    "all_imgs.append(combination_img)\n",
    "input_tensor = backend.concatenate(all_imgs, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun wird das Netzwerk initialisiert. Genaueres zum Aufbau in [diesem Notebook](VGG16_mnist.ipynb). Wir nutzen das bereits mit Daten von Imagenet trainierte Netzwerk und lassen die Fully-Connected-Layers weg, da diese nur für die Klassifikation benötigt werden und keine für uns relevanten Informationen enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'block1_conv1': <tf.Tensor 'block1_conv1_2/Relu:0' shape=(5, 512, 512, 64) dtype=float32>,\n",
       " 'block1_conv2': <tf.Tensor 'block1_conv2_2/Relu:0' shape=(5, 512, 512, 64) dtype=float32>,\n",
       " 'block1_pool': <tf.Tensor 'block1_pool_2/MaxPool:0' shape=(5, 256, 256, 64) dtype=float32>,\n",
       " 'block2_conv1': <tf.Tensor 'block2_conv1_2/Relu:0' shape=(5, 256, 256, 128) dtype=float32>,\n",
       " 'block2_conv2': <tf.Tensor 'block2_conv2_2/Relu:0' shape=(5, 256, 256, 128) dtype=float32>,\n",
       " 'block2_pool': <tf.Tensor 'block2_pool_2/MaxPool:0' shape=(5, 128, 128, 128) dtype=float32>,\n",
       " 'block3_conv1': <tf.Tensor 'block3_conv1_2/Relu:0' shape=(5, 128, 128, 256) dtype=float32>,\n",
       " 'block3_conv2': <tf.Tensor 'block3_conv2_2/Relu:0' shape=(5, 128, 128, 256) dtype=float32>,\n",
       " 'block3_conv3': <tf.Tensor 'block3_conv3_2/Relu:0' shape=(5, 128, 128, 256) dtype=float32>,\n",
       " 'block3_pool': <tf.Tensor 'block3_pool_2/MaxPool:0' shape=(5, 64, 64, 256) dtype=float32>,\n",
       " 'block4_conv1': <tf.Tensor 'block4_conv1_2/Relu:0' shape=(5, 64, 64, 512) dtype=float32>,\n",
       " 'block4_conv2': <tf.Tensor 'block4_conv2_2/Relu:0' shape=(5, 64, 64, 512) dtype=float32>,\n",
       " 'block4_conv3': <tf.Tensor 'block4_conv3_2/Relu:0' shape=(5, 64, 64, 512) dtype=float32>,\n",
       " 'block4_pool': <tf.Tensor 'block4_pool_2/MaxPool:0' shape=(5, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv1': <tf.Tensor 'block5_conv1_2/Relu:0' shape=(5, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv2': <tf.Tensor 'block5_conv2_2/Relu:0' shape=(5, 32, 32, 512) dtype=float32>,\n",
       " 'block5_conv3': <tf.Tensor 'block5_conv3_2/Relu:0' shape=(5, 32, 32, 512) dtype=float32>,\n",
       " 'block5_pool': <tf.Tensor 'block5_pool_2/MaxPool:0' shape=(5, 16, 16, 512) dtype=float32>,\n",
       " 'input_3': <tf.Tensor 'concat_2:0' shape=(5, 512, 512, 3) dtype=float32>}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers = dict([(layer.name, layer.output) for layer in model.layers])\n",
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie man sehen kann, schlägt sich die Bildgröße in allen Schichten in der Parameteranzahl wider. Dies bedeutet, dass eine Erhöhung der Auflösung zu einer deutlich überproportional längeren Berechnungsdauer führt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um ein möglichst gutes Endergebnis zu erhalten, muss dafür gesorgt werden, dass sowohl der Inhalt der Content-Bilder erhalten bleibt, als auch der Style der Style-Bilder angemessen im Ergebnisbild zu erkennen ist. Da diese mit einander in Konflikt stehen, nutzen wir als Loss eine gewichtete Summe. \n",
    "\n",
    "Im Folgenden werden die einzelnen Bestandteile genauer erläutert. \n",
    "\n",
    "Die Werte der Gewichte haben wir aus [Narayanans Notebook](https://github.com/hnarayanan/artistic-style-transfer/blob/master/notebooks/6_Artistic_style_transfer_with_a_repurposed_VGG_Net_16.ipynb) übernommen. Durch viel Ausprobieren fand er diese Werte, die zu einem subjektiv als gut empfundenen Ergebnis geführt haben. Es bedeutet nicht, dass der Style 200x wichtiger als der Content ist, da die Ergebnisse der einzelnen Loss-Funktionen nicht normalisiert werden und somit nicht direkt miteinander vergleichbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 0.025\n",
    "style_weight = 5.0\n",
    "tv_weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = backend.variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Content-Loss ist die euklidische Distanz zwischen den Outputs einer ausgewählten Schicht für den Content und unser Ergebnisbild. \n",
    "\n",
    "TODO: verschiedene Schichten ausprobieren und Ergebnis beschreiben.\n",
    "\n",
    "Die höheren Schichten erkennen eher größere Strukturen und weniger konkrete Details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'Variable_12:0' shape=(1, 512, 512, 3) dtype=float32_ref>, <tf.Variable 'Variable_13:0' shape=(1, 512, 512, 3) dtype=float32_ref>]\n",
      "[<tf.Variable 'Variable_14:0' shape=(1, 512, 512, 3) dtype=float32_ref>, <tf.Variable 'Variable_15:0' shape=(1, 512, 512, 3) dtype=float32_ref>]\n"
     ]
    }
   ],
   "source": [
    "def content_loss(content, combination):\n",
    "    return backend.sum(backend.square(combination - content))\n",
    "\n",
    "# Add content loss to this layer\n",
    "layer_features = layers['block2_conv2']\n",
    "content_features = layer_features[:len(content_imgs) - 1, :, :, :]\n",
    "print(content_imgs)\n",
    "print(style_imgs)\n",
    "combination_features = layer_features[len(content_imgs) + len(style_imgs), :, :, :]\n",
    "\n",
    "loss += content_weight * content_loss(content_features, combination_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Style-Loss ist der interessanteste Teil dieses Algorithmus. Hier wird eine sogenannte Gram-Matrix zwischengeschaltet, die die Korrelation der Outputs der Neuronen verschiedener Schichten für alle Style-Bilder repräsentiert. Somit werden Informationen über die einzelnen Outputs verloren, aber Informationen über die Zusammenhänge der Neuronen untereinander gewonnen. Dies ist ein erstaunlich guter Indikator für Stil-Ähnlichkeit.\n",
    "\n",
    "In der Gram Matrix werden alle Outputs der betroffenen Neuronen paarweise miteinander multipliziert. Dafür wird zunächst ein Spaltenvektor aus allen n Outputs generiert und dieser mit seiner Transposition multipliziert, sodass eine n x n - Matrix entsteht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = backend.dot(features, backend.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = height * width\n",
    "    return backend.sum(backend.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "feature_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3'] # TODO: try out different layers, compare results and write about them\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = layers[layer_name]\n",
    "    for style_img_idx in range(len(style_imgs)):\n",
    "        style_features = layer_features[1 + style_img_idx, :, :, :]\n",
    "        combination_features = layer_features[1 + len(style_imgs), :, :, :]\n",
    "        style_l = style_loss(style_features, combination_features)\n",
    "        loss += (style_weight / (len(feature_layers)*len(style_imgs))) * style_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: try out with and without total variation loss.\n",
    "\n",
    "Damit das Ergebnisbild nicht zu verrauscht ist, wird noch ein dritter Term in die Loss-Funktion aufgenommen: der Total Variation Loss. Hierbei wird einmal das Bild ohne letzte Zeile über das Bild ohne erste Zeile gelegt und die euklidische Distanz berechnet. Für die Spalten wird analog vorgegangen und das Ergebnis aufaddiert. So wird insgesamt jeder Pixel einmal mit jedem direkten Nachbarn (oben, unten, rechts, links) verglichen und zu große Unterschiede werden bestraft. Somit wird einem zu großen Rauschen entgegengewirkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total variation loss to ensure the image is smooth and continuous throughout\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n",
    "    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n",
    "    return backend.sum(backend.pow(a + b, 1.25))\n",
    "\n",
    "loss += tv_weight * total_variation_loss(combination_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all of these variables are nodes in our computational graph, we can directly\n",
    "# calculate the gradients\n",
    "grads = backend.gradients(loss, combination_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [loss]\n",
    "outputs += grads\n",
    "# Create the function from input combination_img to the loss and gradients\n",
    "f_outputs = backend.function([combination_img], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We finally have the gradients and losses at the combination_img computed as variables\n",
    "# and we can use any standard optimization function to optimize combination_img\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, height, width, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1].flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Start of iteration', 0)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.0\n",
    "\n",
    "def transform_back(x):\n",
    "    x1 = copy.deepcopy(x)\n",
    "    x1 = x1.reshape((height, width, 3))\n",
    "    # Convert back from BGR to RGB to display the image\n",
    "    x1 = x1[:, :, ::-1]\n",
    "    x1[:, :, 0] += 103.939\n",
    "    x1[:, :, 1] += 116.779\n",
    "    x1[:, :, 2] += 123.68\n",
    "    x1 = np.clip(x1, 0, 255).astype('uint8')\n",
    "    return Image.fromarray(x1)\n",
    "\n",
    "transform_back(x).save('before.bmp')\n",
    "\n",
    "iters = 2\n",
    "\n",
    "for i in range(iters):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
    "\n",
    "    transform_back(x).save('result' + str(i) + '.bmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artistic-style-transfer",
   "language": "python",
   "name": "artistic-style-transfer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
