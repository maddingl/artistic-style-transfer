{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Art with more then one style image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from keras import backend\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/victor/anaconda3/envs/tf1_tut/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "height = 512\n",
    "width = 512\n",
    "\n",
    "content_img = Image.open('images/hulk.jpg')\n",
    "content_img = content_img.resize((height, width))\n",
    "content_arr = np.asarray(content_img, dtype='float32')\n",
    "content_arr = np.expand_dims(content_arr, axis=0)\n",
    "content_arr[:, :, :, 0] -= 103.939\n",
    "content_arr[:, :, :, 1] -= 116.779\n",
    "content_arr[:, :, :, 2] -= 123.68\n",
    "# Convert from RGB to BGR\n",
    "content_arr = content_arr[:, :, :, ::-1]\n",
    "content_img = backend.variable(content_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content_imgage_paths['hulk.jpg', \"bal\"]\n",
    "content_imgs = []\n",
    "for content_img_path in content_image_paths:\n",
    "    content_img = Image.open(content_img_path)\n",
    "    content_img = content_img.resize((height, width))\n",
    "    content_arr = np.asarray(content_img, dtype='float32')\n",
    "    content_arr = np.expand_dims(content_arr, axis=0)\n",
    "    content_arr[:, :, :, 0] -= 103.939\n",
    "    content_arr[:, :, :, 1] -= 116.779\n",
    "    content_arr[:, :, :, 2] -= 123.68\n",
    "    content_arr = content_arr[:, :, :, ::-1]\n",
    "    content_img = backend.variable(content_arr)\n",
    "    content_imgs.append(content_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_image_paths = ['images/style1.jpg', 'images/style2.jpg']\n",
    "style_imgs = []\n",
    "for style_img_path in style_image_paths:\n",
    "    style_img = Image.open(style_img_path)\n",
    "    style_img = style_img.resize((height, width))\n",
    "    style_arr = np.asarray(style_img, dtype='float32')\n",
    "    style_arr = np.expand_dims(style_arr, axis=0)\n",
    "    style_arr[:, :, :, 0] -= 103.939\n",
    "    style_arr[:, :, :, 1] -= 116.779\n",
    "    style_arr[:, :, :, 2] -= 123.68\n",
    "    # Convert from RGB to BGR\n",
    "    style_arr = style_arr[:, :, :, ::-1]\n",
    "    style_img = backend.variable(style_arr)\n",
    "    style_imgs.append(style_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Channels as the last dimension, using backend Tensorflow\n",
    "combination_img = backend.placeholder((1, height, width, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now finally have the content image variable, style image variables and combination image placeholder\n",
    "# that we will concatenate to build a final input tensor to build our computation graph on top of, essentially\n",
    "# we pass all these inputs to the network as if they are a part of a batch so they are all run parallely and we\n",
    "# use the features generated to modify our combination image based on the losses\n",
    "\n",
    "all_imgs = [content_img]\n",
    "for style_img in style_imgs:\n",
    "    all_imgs.append(style_img)\n",
    "all_imgs.append(combination_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = backend.concatenate(all_imgs, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(input_tensor=input_tensor, weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = dict([(layer.name, layer.output) for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_weight = 0.025\n",
    "style_weight = 5.0\n",
    "tv_weight = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = backend.variable(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
     ]
    }
   ],
   "source": [
    "def content_loss(content, combination):\n",
    "    return backend.sum(backend.square(combination - content))\n",
    "\n",
    "# Add content loss to this layer\n",
    "layer_features = layers['block2_conv2']\n",
    "content_features = layer_features[0, :, :, :]\n",
    "combination_features = layer_features[1 + len(style_imgs), :, :, :]\n",
    "\n",
    "loss += content_weight * content_loss(content_features, combination_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(x):\n",
    "    features = backend.batch_flatten(backend.permute_dimensions(x, (2, 0, 1)))\n",
    "    gram = backend.dot(features, backend.transpose(features))\n",
    "    return gram\n",
    "\n",
    "def style_loss(style, combination):\n",
    "    S = gram_matrix(style)\n",
    "    C = gram_matrix(combination)\n",
    "    channels = 3\n",
    "    size = height * width\n",
    "    return backend.sum(backend.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n",
    "\n",
    "feature_layers = ['block1_conv2', 'block2_conv2', 'block3_conv3', 'block4_conv3', 'block5_conv3']\n",
    "for layer_name in feature_layers:\n",
    "    layer_features = layers[layer_name]\n",
    "    for style_img_idx in range(len(style_imgs)):\n",
    "        style_features = layer_features[1 + style_img_idx, :, :, :]\n",
    "        combination_features = layer_features[1 + len(style_imgs), :, :, :]\n",
    "        style_l = style_loss(style_features, combination_features)\n",
    "        loss += (style_weight / (len(feature_layers)*len(style_imgs))) * style_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total variation loss to ensure the image is smooth and continuous throughout\n",
    "\n",
    "def total_variation_loss(x):\n",
    "    a = backend.square(x[:, :height-1, :width-1, :] - x[:, 1:, :width-1, :])\n",
    "    b = backend.square(x[:, :height-1, :width-1, :] - x[:, :height-1, 1:, :])\n",
    "    return backend.sum(backend.pow(a + b, 1.25))\n",
    "\n",
    "loss += tv_weight * total_variation_loss(combination_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all of these variables are nodes in our computational graph, we can directly\n",
    "# calculate the gradients\n",
    "grads = backend.gradients(loss, combination_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [loss]\n",
    "outputs += grads\n",
    "# Create the function from input combination_img to the loss and gradients\n",
    "f_outputs = backend.function([combination_img], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We finally have the gradients and losses at the combination_img computed as variables\n",
    "# and we can use any standard optimization function to optimize combination_img\n",
    "\n",
    "def eval_loss_and_grads(x):\n",
    "    x = x.reshape((1, height, width, 3))\n",
    "    outs = f_outputs([x])\n",
    "    loss_value = outs[0]\n",
    "    grad_values = outs[1].flatten().astype('float64')\n",
    "    return loss_value, grad_values\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        self.loss_value = None\n",
    "        self.grads_values = None\n",
    "    def loss(self, x):\n",
    "        assert self.loss_value is None\n",
    "        loss_value, grad_values = eval_loss_and_grads(x)\n",
    "        self.loss_value = loss_value\n",
    "        self.grad_values = grad_values\n",
    "        return self.loss_value\n",
    "    def grads(self, x):\n",
    "        assert self.loss_value is not None\n",
    "        grad_values = np.copy(self.grad_values)\n",
    "        self.loss_value = None\n",
    "        self.grad_values = None\n",
    "        return grad_values\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of iteration 0\n",
      "Current loss value: 867278850000.0\n",
      "Iteration 0 completed in 488s\n",
      "Start of iteration 1\n",
      "Current loss value: 819821300000.0\n",
      "Iteration 1 completed in 532s\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(0, 255, (1, height, width, 3)) - 128.0\n",
    "\n",
    "iters = 2\n",
    "\n",
    "for i in range(iters):\n",
    "    print('Start of iteration', i)\n",
    "    start_time = time.time()\n",
    "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(), fprime=evaluator.grads, maxfun=20)\n",
    "    print('Current loss value:', min_val)\n",
    "    end_time = time.time()\n",
    "    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n",
    "\n",
    "    x1 = copy.deepcopy(x)\n",
    "    x1 = x1.reshape((height, width, 3))\n",
    "    # Convert back from BGR to RGB to display the image\n",
    "    x1 = x1[:, :, ::-1]\n",
    "    x1[:, :, 0] += 103.939\n",
    "    x1[:, :, 1] += 116.779\n",
    "    x1[:, :, 2] += 123.68\n",
    "    x1 = np.clip(x1, 0, 255).astype('uint8')\n",
    "    img_final = Image.fromarray(x1)\n",
    "    img_final.save('result' + str(i) + '.bmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1_tut] *",
   "language": "python",
   "name": "conda-env-tf1_tut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
